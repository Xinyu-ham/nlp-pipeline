{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Modeling (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdata.datapipes.iter import IterableWrapper, IterDataPipe\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import logging\n",
    "\n",
    "\n",
    "import boto3, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "BUCKET_NAME = 'xy-mp-pipeline'\n",
    "METADATA_KEY = 'data/covid-csv-metadata.json'\n",
    "bucket = boto3.resource('s3').Bucket(BUCKET_NAME)\n",
    "metadata = json.loads(bucket.Object(METADATA_KEY).get()['Body'].read())\n",
    "input_schema = metadata['schema']['input_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUTPUT_PATH = 'data/covid-csv'\n",
    "N_SAMPLES = metadata['dataset_size']\n",
    "TRAIN_FILES = N_SAMPLES * 4 // 5 // 16 + 1\n",
    "TEST_FILES = N_SAMPLES // 5 // 16 + 1\n",
    "BATCH_SIZE = metadata['batch_size']\n",
    "TRAIN_S3_URL = f's3://{BUCKET_NAME}/{OUTPUT_PATH}/training/'\n",
    "TEST_S3_URL = f's3://{BUCKET_NAME}/{OUTPUT_PATH}/testing/'\n",
    "TEST_DATASET_SIZE = metadata['test_size']\n",
    "TRAIN_DATASET_SIZE = metadata['train_size']\n",
    "MODEL_OUTPUT_PATH = 'assets/model'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data pipe\n",
    "I'm sure most of PyTorch users are already familiar with Datasets, it is a convenient way to load data into memory. This time, we will use a different approach to load data into memory, which is called data pipe. Data pipe is a new feature introduced in PyTorch 1.8.0, it is a new way to load data into memory, and it is more flexible than Datasets. \n",
    "\n",
    "The main benefit for this particular project is that we can load data from a cloud bucket, one batch at a time, and we can also do some preprocessing on the fly. This is very useful when we have a large dataset and we don't want to load all the data into memory at once.\n",
    "\n",
    "With this, we can also easily perform data parallel training, which is a very useful technique when we have a large dataset and we want to train our model faster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall the dataframe**\n",
    ">\n",
    "    RangeIndex: 19454 entries, 0 to 19453\n",
    "    Data columns (total 22 columns):\n",
    "    #   Column           Non-Null Count  Dtype \n",
    "    ---  ------           --------------  ----- \n",
    "    0   headlines        19454 non-null  object\n",
    "    1   length           19454 non-null  int64 \n",
    "    2   has_num          19454 non-null  bool  \n",
    "    3   ner_percent      19454 non-null  int64 \n",
    "    4   ner_quantity     19454 non-null  int64 \n",
    "    5   ner_law          19454 non-null  int64 \n",
    "    6   ner_person       19454 non-null  int64 \n",
    "    7   ner_product      19454 non-null  int64 \n",
    "    8   ner_gpe          19454 non-null  int64 \n",
    "    9   ner_work_of_art  19454 non-null  int64 \n",
    "    10  ner_date         19454 non-null  int64 \n",
    "    11  ner_time         19454 non-null  int64 \n",
    "    12  ner_cardinal     19454 non-null  int64 \n",
    "    13  ner_org          19454 non-null  int64 \n",
    "    14  ner_money        19454 non-null  int64 \n",
    "    15  ner_language     19454 non-null  int64 \n",
    "    16  ner_ordinal      19454 non-null  int64 \n",
    "    17  ner_event        19454 non-null  int64 \n",
    "    18  ner_loc          19454 non-null  int64 \n",
    "    19  ner_fac          19454 non-null  int64 \n",
    "    20  ner_norp         19454 non-null  int64 \n",
    "    21  outcome          19454 non-null  int64 \n",
    "    dtypes: bool(1), int64(20), object(1)\n",
    "    memory usage: 3.1+ MB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we want to pass the text into a pre-trained BERT model, at the same time, we also want to pass the other features into a fully connected layer. To do this, we need to create a custom data pipe that gives the input to BERT, tabular features, and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(IterDataPipe):\n",
    "    def __init__(self, s3_data_path, tokenizer, num_files):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.url_wrapper = IterableWrapper([s3_data_path]).list_files_by_s3().shuffle().sharding_filter()\n",
    "        self.num_files = num_files\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _, file in self.url_wrapper.load_files_by_s3():\n",
    "            temp = pd.read_csv(file)\n",
    "            label = torch.from_numpy(temp['outcome'].values)\n",
    "            # For BERT model\n",
    "            bert_input = []\n",
    "            embedded = [self.tokenizer(t, padding='max_length', max_length=100, truncation=True, return_tensors='pt') for t in temp['headlines']]\n",
    "            bert_input.append(torch.cat([e['input_ids'] for e in embedded], dim=0))\n",
    "            bert_input.append(torch.cat([e['attention_mask'] for e in embedded], dim=0))\n",
    "\n",
    "            # Tabular features\n",
    "            tabular_input = [torch.from_numpy(temp[col].values).to(torch.float32).squeeze() for col in temp.columns if col not in ['outcome', 'headlines']]\n",
    "            yield bert_input, tabular_input, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start defining our model that extends from a pre-trained BERT model. This will give us a model that can take in text input and output a vector representation of the text. We can then combine this with a fully connected layer that takes in the tabular features and the vector representation of the text, and output a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class FakeNewsClassifier(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.dropout_1 = torch.nn.Dropout(0.25)\n",
    "        self.linear = torch.nn.Linear(768, 12)\n",
    "        self.dropout_2 = torch.nn.Dropout(0.25)\n",
    "        self.final_linear = torch.nn.Linear(32, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.normalize = torch.nn.functional.normalize\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, bert_input: dict, tabular_input: list):\n",
    "        print(bert_input['input_ids'].shape, bert_input['attention_mask'].shape, tabular_input.shape)\n",
    "        _, pooled_output = self.bert(**bert_input)\n",
    "        dropout_1_output = self.dropout_1(pooled_output)\n",
    "        linear_output = self.linear(dropout_1_output)\n",
    "        relu_output = self.relu(linear_output)\n",
    "        norm1 = self.normalize(relu_output, p=2, dim=1)\n",
    "        norm2 = self.normalize(tabular_input, p=2, dim=1)\n",
    "        combined_output = torch.cat([norm1, norm2], dim=1)\n",
    "        dropout_2_output = self.dropout_2(combined_output)\n",
    "        final_output = self.final_linear(dropout_2_output)\n",
    "        return self.sigmoid(final_output)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the model looks like"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/model_architecture.png \"Model Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(pretrained_model_name: str, train_data_url: str, test_data_url: str, train_len: int, test_len:int, train_file_len: int, test_file_len: int, epochs: int, lr: float):\n",
    "    # Prepare dataloaders\n",
    "    model = FakeNewsClassifier(pretrained_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "    train_df = TextDataset(train_data_url, tokenizer, train_file_len)\n",
    "    test_df = TextDataset(test_data_url, tokenizer, test_file_len)\n",
    "\n",
    "    train_loader = DataLoader(train_df, batch_size=1, shuffle=True)\n",
    "    test_loader = DataLoader(test_df, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Config device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Config optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = torch.nn.BCELoss()\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        training_acc = 0.0\n",
    "\n",
    "        for bert_input, tabular_input, label in tqdm(train_loader):\n",
    "            bert_input = {\n",
    "                'input_ids': bert_input[0].squeeze().to(device),\n",
    "                'attention_mask': bert_input[1].squeeze().to(device),\n",
    "                'return_dict': False\n",
    "            }\n",
    "            tabular_input = torch.cat(tabular_input).T.to(device)\n",
    "            label = label.T.to(device)\n",
    "\n",
    "            output = model(bert_input, tabular_input)\n",
    "\n",
    "            loss = loss_function(output, label.float())\n",
    "            training_loss += loss.item()\n",
    "\n",
    "            # get acc of signmoid output\n",
    "            acc = (output[0].round() == label).sum().item()\n",
    "            training_acc += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        validation_loss = 0.0\n",
    "        validation_acc = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for bert_input, tabular_input, label in test_loader:\n",
    "                bert_input = {\n",
    "                    'input_ids': bert_input[0].squeeze().to(device),\n",
    "                    'attention_mask': bert_input[1].squeeze().to(device),\n",
    "                    'return_dict': False\n",
    "                }\n",
    "                tabular_input = torch.cat(tabular_input).T.to(device)\n",
    "                label = label.T.to(device)\n",
    "\n",
    "                output = model(bert_input, tabular_input)\n",
    "\n",
    "                loss = loss_function(output, label.float())\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # get acc of signmoid output\n",
    "                acc = (output[0].round() == label).sum().item()\n",
    "                validation_acc += acc\n",
    "        print(f'Epoch: {epoch+1}/{epochs} | Training loss: {training_loss/train_len:.3f} | Training acc: {training_acc/train_len:.3f} | Validation loss: {validation_loss/test_len:.3f} | Validation acc: {validation_acc/test_len:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 973/973 [03:28<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 | Training loss: 0.038 | Training acc: 0.871 | Validation loss: 0.038 | Validation acc: 0.883\n",
      "Model saved to:   assets/model\n"
     ]
    }
   ],
   "source": [
    "pretrain_name = 'bert-base-uncased'\n",
    "model = FakeNewsClassifier(pretrain_name)\n",
    "EPOCHS = 1\n",
    "LR = 5e-6\n",
    "\n",
    "train_model(\n",
    "    pretrain_name, \n",
    "    TRAIN_S3_URL, \n",
    "    TEST_S3_URL, \n",
    "    TRAIN_DATASET_SIZE, \n",
    "    TEST_DATASET_SIZE, \n",
    "    TRAIN_FILES, \n",
    "    TEST_FILES,\n",
    "    EPOCHS, \n",
    "    LR\n",
    ")\n",
    "\n",
    "import os\n",
    "if not os.path.exists(MODEL_OUTPUT_PATH):\n",
    "    os.makedirs(MODEL_OUTPUT_PATH)\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_OUTPUT_PATH + '/baseline.pth')\n",
    "print('Model saved to:  ', MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer\n",
    "pretrain_name = 'bert-base-uncased'\n",
    "# load model from output path\n",
    "model = FakeNewsClassifier(pretrain_name)\n",
    "model.load_state_dict(torch.load(MODEL_OUTPUT_PATH + '/baseline.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure F1 score from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [03:42<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8316017316017316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# from sklearn.metrics import f1_score\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrain_name)\n",
    "# test_ds = TextDataset(TEST_S3_URL, tokenizer, TEST_FILES)\n",
    "# test_loader = DataLoader(test_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = []\n",
    "#     labels = []\n",
    "#     for bert_input, tabular_input, label in tqdm(test_loader):\n",
    "#         bert_input = {\n",
    "#             'input_ids': bert_input[0].squeeze(),\n",
    "#             'attention_mask': bert_input[1].squeeze(),\n",
    "#             'return_dict': False\n",
    "#         }\n",
    "#         tabular_input = torch.cat(tabular_input).T\n",
    "#         label = label.T\n",
    "\n",
    "#         outputs.append(model(bert_input, tabular_input))\n",
    "#         labels.append(label)\n",
    "\n",
    "# output = torch.cat(outputs, dim=0)\n",
    "# label = torch.cat(labels, dim=0)\n",
    "# print(f'F1 score: {f1_score(output.round().detach().numpy(), label.detach().numpy())}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make inference with model and user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_name)\n",
    "\n",
    "def preprocess_headline(headline: str):\n",
    "    tokens = tokenizer(headline, return_tensors='pt', padding='max_length', truncation=True, max_length=64)\n",
    "    tokens = {k: v.squeeze() for k, v in tokens.items()}\n",
    "    input_ids = tokens['input_ids'].unsqueeze(0)\n",
    "    attention_mask = tokens['attention_mask'].unsqueeze(0)\n",
    "    bert_input = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'return_dict': False\n",
    "    }\n",
    "\n",
    "\n",
    "    headline_len = len(headline.split())\n",
    "    has_stats = int(re.match(r'\\d', headline) is not None)\n",
    "\n",
    "    ner_count = [f'ner_{ent.label_}' for ent in nlp(headline).ents]\n",
    "    ner_count = Counter(ner_count)\n",
    "    ner_input = get_ner_input(ner_count, input_schema)\n",
    "    return bert_input, torch.cat([torch.tensor([headline_len, has_stats]), ner_input], dim=0).unsqueeze(0)\n",
    "\n",
    "    \n",
    "def get_ner_input(ner_count, input_schema):\n",
    "    ner_features = [s for s in input_schema if s.startswith('ner_')]\n",
    "    ner_input = torch.zeros(len(ner_features))\n",
    "    for i, feat in enumerate(ner_features):\n",
    "        ner_input[i] = ner_count.get(feat, 0)\n",
    "    return ner_input\n",
    "    \n",
    "\n",
    "def classify(headline: str):\n",
    "    bert_input, tabular_input = preprocess_headline(headline)\n",
    "    with torch.no_grad():\n",
    "        output = model(bert_input, tabular_input)\n",
    "    return output.round().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6336e33fd1414fa18b03edf42f842cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Headline:', layout=Layout(display='flex', flex_flow='column wrap', height='69p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c3c73b945941db9de95b1332a0c3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Button(description='Fact Check', layout=Layout(display='flex'), style=ButtonStyle()), Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# notebook text input ui\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "text = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type something',\n",
    "    description='Headline:',\n",
    "    disabled=False,\n",
    "    layout={'width': '69%', 'height': '69px', 'display': 'flex', 'flex_flow': 'column wrap'}\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Fact Check\", layout={'display': 'flex'})\n",
    "out = widgets.Label(value='Click button to fact check headline!', layout={'display': 'flex', 'flex_flow': 'column wrap', 'align_items': 'flex-end'})\n",
    "\n",
    "def fact_check(b):\n",
    "    headline = text.value\n",
    "    pred = classify(headline)\n",
    "    \n",
    "    if pred == 1:\n",
    "        \n",
    "        out.value = 'This is fake news!'\n",
    "    else:\n",
    "        out.value = 'This is real news!'\n",
    "\n",
    "button.on_click(fact_check)\n",
    "\n",
    "vb = widgets.Box([button, out])\n",
    "display(text)\n",
    "display(vb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
