{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Modeling (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdata.datapipes.iter import IterableWrapper, IterDataPipe\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'xy-mp-pipeline'\n",
    "OUTPUT_PATH = 'data/covid-csv'\n",
    "N_SAMPLES = 19454\n",
    "TRAIN_FILES = N_SAMPLES * 4 // 5 // 16 + 1\n",
    "TEST_FILES = N_SAMPLES // 5 // 16 + 1\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_S3_URL = f's3://{BUCKET_NAME}/{OUTPUT_PATH}/training/'\n",
    "TEST_S3_URL = f's3://{BUCKET_NAME}/{OUTPUT_PATH}/testing/'\n",
    "TEST_DATASET_SIZE = N_SAMPLES // 5\n",
    "TRAIN_DATASET_SIZE = N_SAMPLES - TEST_DATASET_SIZE\n",
    "MODEL_OUTPUT_PATH = 'assets/model'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data pipe\n",
    "I'm sure most of PyTorch users are already familiar with Datasets, it is a convenient way to load data into memory. This time, we will use a different approach to load data into memory, which is called data pipe. Data pipe is a new feature introduced in PyTorch 1.8.0, it is a new way to load data into memory, and it is more flexible than Datasets. \n",
    "\n",
    "The main benefit for this particular project is that we can load data from a cloud bucket, one batch at a time, and we can also do some preprocessing on the fly. This is very useful when we have a large dataset and we don't want to load all the data into memory at once.\n",
    "\n",
    "With this, we can also easily perform data parallel training, which is a very useful technique when we have a large dataset and we want to train our model faster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall the dataframe**\n",
    ">\n",
    "    RangeIndex: 19454 entries, 0 to 19453\n",
    "    Data columns (total 22 columns):\n",
    "    #   Column           Non-Null Count  Dtype \n",
    "    ---  ------           --------------  ----- \n",
    "    0   headlines        19454 non-null  object\n",
    "    1   length           19454 non-null  int64 \n",
    "    2   has_num          19454 non-null  bool  \n",
    "    3   ner_percent      19454 non-null  int64 \n",
    "    4   ner_quantity     19454 non-null  int64 \n",
    "    5   ner_law          19454 non-null  int64 \n",
    "    6   ner_person       19454 non-null  int64 \n",
    "    7   ner_product      19454 non-null  int64 \n",
    "    8   ner_gpe          19454 non-null  int64 \n",
    "    9   ner_work_of_art  19454 non-null  int64 \n",
    "    10  ner_date         19454 non-null  int64 \n",
    "    11  ner_time         19454 non-null  int64 \n",
    "    12  ner_cardinal     19454 non-null  int64 \n",
    "    13  ner_org          19454 non-null  int64 \n",
    "    14  ner_money        19454 non-null  int64 \n",
    "    15  ner_language     19454 non-null  int64 \n",
    "    16  ner_ordinal      19454 non-null  int64 \n",
    "    17  ner_event        19454 non-null  int64 \n",
    "    18  ner_loc          19454 non-null  int64 \n",
    "    19  ner_fac          19454 non-null  int64 \n",
    "    20  ner_norp         19454 non-null  int64 \n",
    "    21  outcome          19454 non-null  int64 \n",
    "    dtypes: bool(1), int64(20), object(1)\n",
    "    memory usage: 3.1+ MB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we want to pass the text into a pre-trained BERT model, at the same time, we also want to pass the other features into a fully connected layer. To do this, we need to create a custom data pipe that gives the input to BERT, tabular features, and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(IterDataPipe):\n",
    "    def __init__(self, s3_urls, tokenizer, num_files):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.url_wrapper = s3_urls\n",
    "        self.num_files = num_files\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _, file in self.url_wrapper.load_files_by_s3():\n",
    "            temp = pd.read_csv(file)\n",
    "            label = torch.from_numpy(temp['outcome'].values)\n",
    "            # For BERT model\n",
    "            bert_input = []\n",
    "            embedded = [self.tokenizer(t, padding='max_length', max_length=100, truncation=True, return_tensors='pt') for t in temp['headlines']]\n",
    "            bert_input.append(torch.cat([e['input_ids'] for e in embedded], dim=0))\n",
    "            bert_input.append(torch.cat([e['attention_mask'] for e in embedded], dim=0))\n",
    "\n",
    "            # Tabular features\n",
    "            tabular_input = [torch.from_numpy(temp[col].values).to(torch.float32).squeeze() for col in temp.columns if col not in ['outcome', 'headlines']]\n",
    "            yield bert_input, tabular_input, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "[torch.Size([1, 16, 100]), torch.Size([1, 16, 100])]\n",
      "[torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16])]\n",
      "torch.Size([1, 16])\n",
      "Iter: 1\n",
      "[torch.Size([1, 16, 100]), torch.Size([1, 16, 100])]\n",
      "[torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16])]\n",
      "torch.Size([1, 16])\n",
      "Iter: 2\n",
      "[torch.Size([1, 16, 100]), torch.Size([1, 16, 100])]\n",
      "[torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16])]\n",
      "torch.Size([1, 16])\n",
      "Iter: 3\n",
      "[torch.Size([1, 16, 100]), torch.Size([1, 16, 100])]\n",
      "[torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16])]\n",
      "torch.Size([1, 16])\n",
      "Iter: 4\n",
      "[torch.Size([1, 16, 100]), torch.Size([1, 16, 100])]\n",
      "[torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16]), torch.Size([1, 16])]\n",
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "train_data_url = f's3://{BUCKET_NAME}/{OUTPUT_PATH}/training/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_len = N_SAMPLES // 2\n",
    "train_s3_url = IterableWrapper([train_data_url]).list_files_by_s3().shuffle().sharding_filter()\n",
    "train_df = TextDataset(train_s3_url, tokenizer, train_len)\n",
    "train_loader = DataLoader(train_df, batch_size=1, shuffle=True)\n",
    "\n",
    "cnt = 0\n",
    "for bert_input, tabular_input, output in train_loader:\n",
    "    print(f'Iter: {cnt}')\n",
    "    print([x.shape for x in bert_input])\n",
    "    print([x.shape for x in tabular_input])\n",
    "    print(output.shape)\n",
    "    cnt += 1\n",
    "    if cnt == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class FakeNewsClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout_1 = torch.nn.Dropout(0.25)\n",
    "        self.linear = torch.nn.Linear(768, 12)\n",
    "        self.dropout_2 = torch.nn.Dropout(0.25)\n",
    "        self.final_linear = torch.nn.Linear(32, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, bert_input: dict, tabular_input: list):\n",
    "        _, pooled_output = self.bert(**bert_input)\n",
    "        dropout_1_output = self.dropout_1(pooled_output)\n",
    "        linear_output = self.linear(dropout_1_output)\n",
    "        norm1 = torch.nn.functional.normalize(linear_output, p=2, dim=1)\n",
    "        norm2 = torch.nn.functional.normalize(tabular_input, p=2, dim=1)\n",
    "        combined_output = torch.cat([norm1, norm2], dim=1)\n",
    "        dropout_2_output = self.dropout_2(combined_output)\n",
    "        final_output = self.final_linear(dropout_2_output)\n",
    "        return self.sigmoid(final_output)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the model looks like"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/model_architecture.png.png \"Model Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: torch.nn.Module, train_data_url: str, test_data_url: str, train_len: int, test_len:int, train_file_len: int, test_file_len: int, epochs: int, lr: float):\n",
    "    # Prepare dataloaders\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    train_s3_url = IterableWrapper([train_data_url]).list_files_by_s3().shuffle().sharding_filter()\n",
    "    test_s3_url = IterableWrapper([test_data_url]).list_files_by_s3().shuffle().sharding_filter()\n",
    "\n",
    "    train_df = TextDataset(train_s3_url, tokenizer, train_file_len)\n",
    "    test_df = TextDataset(test_s3_url, tokenizer, test_file_len)\n",
    "\n",
    "    train_loader = DataLoader(train_df, batch_size=1, shuffle=True)\n",
    "    test_loader = DataLoader(test_df, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Config device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Config optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = torch.nn.BCELoss()\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        training_acc = 0.0\n",
    "\n",
    "        for bert_input, tabular_input, label in tqdm(train_loader):\n",
    "            bert_input = {\n",
    "                'input_ids': bert_input[0].squeeze().to(device),\n",
    "                'attention_mask': bert_input[1].squeeze().to(device),\n",
    "                'return_dict': False\n",
    "            }\n",
    "            tabular_input = torch.cat(tabular_input).T.to(device)\n",
    "            label = label.T.to(device)\n",
    "\n",
    "            output = model(bert_input, tabular_input)\n",
    "\n",
    "            loss = loss_function(output, label.float())\n",
    "            training_loss += loss.item()\n",
    "\n",
    "            # get acc of signmoid output\n",
    "            acc = (output[0].round() == label).sum().item()\n",
    "            training_acc += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        validation_loss = 0.0\n",
    "        validation_acc = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for bert_input, tabular_input, label in train_loader:\n",
    "                bert_input = {\n",
    "                    'input_ids': bert_input[0].squeeze().to(device),\n",
    "                    'attention_mask': bert_input[1].squeeze().to(device),\n",
    "                    'return_dict': False\n",
    "                }\n",
    "                tabular_input = torch.cat(tabular_input).T.to(device)\n",
    "                label = label.T.to(device)\n",
    "\n",
    "                output = model(bert_input, tabular_input)\n",
    "\n",
    "                loss = loss_function(output, label.float())\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # get acc of signmoid output\n",
    "                acc = (output[0].round() == label).sum().item()\n",
    "                validation_acc += acc\n",
    "        print(f'Epoch: {epoch+1}/{epochs} | Training loss: {training_loss/train_len:.3f} | Training acc: {training_acc/train_len:.3f} | Validation loss: {validation_loss/test_len:.3f} | Validation acc: {validation_acc/test_len:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/15564 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/15564 [00:03<13:25:11,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/15564 [00:05<11:58:08,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/15564 [00:08<11:32:10,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/15564 [00:10<11:22:00,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/15564 [00:13<11:13:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/15564 [00:15<11:10:01,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/15564 [00:18<11:08:27,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/15564 [00:20<11:02:17,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/15564 [00:23<10:59:32,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/15564 [00:25<10:57:58,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/15564 [00:28<10:57:21,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/15564 [00:31<10:57:52,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/15564 [00:33<10:56:39,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/15564 [00:35<11:53:33,  2.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Xin Yu\\Desktop\\data_science\\nlp-pipeline\\model.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test_len \u001b[39m=\u001b[39m N_SAMPLES \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_len \u001b[39m=\u001b[39m N_SAMPLES \u001b[39m-\u001b[39m test_len\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_model(model, TRAIN_S3_URL, TEST_S3_URL, train_len, test_len, EPOCHS, LR)\n",
      "\u001b[1;32mc:\\Users\\Xin Yu\\Desktop\\data_science\\nlp-pipeline\\model.ipynb Cell 13\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_data_url, test_data_url, train_len, test_len, epochs, lr)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     training_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m acc\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m validation_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Xin Yu\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Xin Yu\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = FakeNewsClassifier()\n",
    "EPOCHS = 5\n",
    "LR = 5e-6\n",
    "\n",
    "\n",
    "train_model(\n",
    "    model, \n",
    "    TRAIN_S3_URL, \n",
    "    TEST_S3_URL, \n",
    "    TRAIN_DATASET_SIZE, \n",
    "    TEST_DATASET_SIZE, \n",
    "    TRAIN_FILES, \n",
    "    TEST_FILES,\n",
    "    EPOCHS, \n",
    "    LR\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_OUTPUT_PATH + '/baseline.pth')\n",
    "print('Model saved to:  ', MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = df[df['outcome'] == 1].headlines.values\n",
    "tests = [tokenizer(t, padding='max_length', max_length=100, truncation=True, return_tensors='pt') for t in tests]\n",
    "input_ids = torch.cat([e['input_ids'] for e in tests], dim=0)\n",
    "attention_mask = torch.cat([e['attention_mask'] for e in tests], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973\n",
      "973\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_s3_url = IterableWrapper([TRAIN_S3_URL]).list_files_by_s3().shuffle().sharding_filter()\n",
    "test_s3_url = IterableWrapper([TEST_S3_URL]).list_files_by_s3().shuffle().sharding_filter()\n",
    "\n",
    "train_df = TextDataset(train_s3_url, tokenizer, train_len)\n",
    "test_df = TextDataset(test_s3_url, tokenizer, test_len)\n",
    "\n",
    "train_loader = DataLoader(train_df, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_df, batch_size=1, shuffle=True)\n",
    "\n",
    "i = 0\n",
    "for bert_input, tabular_input, label in train_loader:\n",
    "    i += 1\n",
    "\n",
    "print(i)\n",
    "i = 0\n",
    "\n",
    "for bert_input, tabular_input, label in test_loader:\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
