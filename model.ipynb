{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchdata.datapipes.iter import IterableWrapper, S3FileLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = 'data/covid.zip'\n",
    "BUCKET_NAME = 'xy-mp-pipeline'\n",
    "OUTPUT_PATH = 'data/covid-csv'\n",
    "N_SAMPLES = 10201\n",
    "BATCHES = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and uploading data to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A post claims compulsory vacination violates t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A photo claims that this person is a doctor wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Post about a video claims that it is a protest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All deaths by respiratory failure and pneumoni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The dean of the College of Biologists of Euska...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  outcome\n",
       "0  A post claims compulsory vacination violates t...        0\n",
       "1  A photo claims that this person is a doctor wh...        0\n",
       "2  Post about a video claims that it is a protest...        0\n",
       "3  All deaths by respiratory failure and pneumoni...        0\n",
       "4  The dean of the College of Biologists of Euska...        0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_SOURCE)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df, batches):\n",
    "    len_positive = len(df[df['outcome'] == 1])\n",
    "    len_negative = len(df[df['outcome'] == 0])\n",
    "\n",
    "    pad_positve = batches - len_positive % batches\n",
    "    pad_negative = batches - len_negative % batches\n",
    "\n",
    "    df = pd.concat([df, df[df['outcome'] == 1].sample(n=pad_positve, replace=True)], axis=0)\n",
    "    df = pd.concat([df, df[df['outcome'] == 1].sample(n=pad_negative, replace=True)], axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=0.2):\n",
    "    positive = df[df['outcome'] == 1]\n",
    "    negative = df[df['outcome'] == 0]\n",
    "\n",
    "    n_pos = len(positive)\n",
    "    n_neg = len(negative)\n",
    "\n",
    "    negative_test = negative.iloc[:int(n_neg * test_size)]\n",
    "    negative_train = negative.iloc[int(n_neg * test_size):]\n",
    "    positive_test = positive.iloc[:int(n_pos * test_size)]\n",
    "    positive_train = positive.iloc[int(n_pos * test_size):]\n",
    "\n",
    "    train_df = pd.concat([positive_train, negative_train])\n",
    "    test_df = pd.concat([positive_test, negative_test])\n",
    "    return train_df, test_df\n",
    " \n",
    "def write_csvs(df, folder, output_path, batches):\n",
    "    if not os.path.exists(output_path + '/' + folder):\n",
    "        os.makedirs(output_path + '/' + folder)\n",
    "\n",
    "    batch_size = len(df) // batches\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        batch.to_csv(f'{output_path}/{folder}/file{i}.csv', index=False)\n",
    "\n",
    "def write_files_to_s3(output_path, bucket_name):\n",
    "    !aws s3 rm --recursive s3://$bucket_name/$output_path\n",
    "    !aws s3 cp --recursive $output_path s3://$bucket_name/$output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1134.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1071.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file0.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1008.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1323.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file126.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1386.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1449.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1260.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1512.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1197.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1575.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1638.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1827.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1701.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file189.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1890.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1764.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file1953.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file2016.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file315.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file252.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file441.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file378.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file63.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file504.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file567.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file630.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file693.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file819.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file756.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file882.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/testing/file945.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file1275.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file1785.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file1530.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file2550.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file2295.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file1020.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file255.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file0.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file2040.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file3315.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file3060.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file4590.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file2805.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file3570.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file4080.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file4845.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file3825.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file4335.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file510.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file5100.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file5865.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file5610.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file6120.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file7140.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file6375.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file5355.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file6630.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file6885.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file7395.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file765.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file7650.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file7905.csv\n",
      "delete: s3://xy-mp-pipeline/data/covid-csv/training/file8160.csv\n",
      "Completed 102.9 KiB/16.0 MiB (178.7 KiB/s) with 66 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1071.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1071.csv\n",
      "Completed 102.9 KiB/16.0 MiB (178.7 KiB/s) with 65 file(s) remaining\n",
      "Completed 206.2 KiB/16.0 MiB (347.9 KiB/s) with 65 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1134.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1134.csv\n",
      "Completed 206.2 KiB/16.0 MiB (347.9 KiB/s) with 64 file(s) remaining\n",
      "Completed 308.2 KiB/16.0 MiB (506.7 KiB/s) with 64 file(s) remaining\n",
      "Completed 411.4 KiB/16.0 MiB (675.3 KiB/s) with 64 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1323.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1323.csv\n",
      "Completed 411.4 KiB/16.0 MiB (675.3 KiB/s) with 63 file(s) remaining\n",
      "Completed 510.1 KiB/16.0 MiB (837.2 KiB/s) with 63 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1575.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1575.csv\n",
      "Completed 510.1 KiB/16.0 MiB (837.2 KiB/s) with 62 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1260.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1260.csv\n",
      "Completed 510.1 KiB/16.0 MiB (837.2 KiB/s) with 61 file(s) remaining\n",
      "Completed 612.4 KiB/16.0 MiB (1003.4 KiB/s) with 61 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1197.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1197.csv\n",
      "Completed 612.4 KiB/16.0 MiB (1003.4 KiB/s) with 60 file(s) remaining\n",
      "Completed 715.1 KiB/16.0 MiB (1.1 MiB/s) with 60 file(s) remaining   \n",
      "upload: data\\covid-csv\\testing\\file126.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file126.csv\n",
      "Completed 715.1 KiB/16.0 MiB (1.1 MiB/s) with 59 file(s) remaining\n",
      "Completed 814.0 KiB/16.0 MiB (1.3 MiB/s) with 59 file(s) remaining\n",
      "Completed 915.4 KiB/16.0 MiB (1.5 MiB/s) with 59 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1386.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1386.csv\n",
      "Completed 915.4 KiB/16.0 MiB (1.5 MiB/s) with 58 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1449.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1449.csv\n",
      "Completed 915.4 KiB/16.0 MiB (1.5 MiB/s) with 57 file(s) remaining\n",
      "Completed 1014.4 KiB/16.0 MiB (1.6 MiB/s) with 57 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1512.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1512.csv\n",
      "Completed 1014.4 KiB/16.0 MiB (1.6 MiB/s) with 56 file(s) remaining\n",
      "Completed 1.1 MiB/16.0 MiB (1.7 MiB/s) with 56 file(s) remaining   \n",
      "upload: data\\covid-csv\\testing\\file1638.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1638.csv\n",
      "Completed 1.1 MiB/16.0 MiB (1.7 MiB/s) with 55 file(s) remaining\n",
      "Completed 1.1 MiB/16.0 MiB (1.7 MiB/s) with 55 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file2016.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file2016.csv\n",
      "Completed 1.1 MiB/16.0 MiB (1.7 MiB/s) with 54 file(s) remaining\n",
      "Completed 1.2 MiB/16.0 MiB (1.8 MiB/s) with 54 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1701.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1701.csv\n",
      "Completed 1.2 MiB/16.0 MiB (1.8 MiB/s) with 53 file(s) remaining\n",
      "Completed 1.3 MiB/16.0 MiB (1.9 MiB/s) with 53 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1953.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1953.csv\n",
      "Completed 1.3 MiB/16.0 MiB (1.9 MiB/s) with 52 file(s) remaining\n",
      "Completed 1.4 MiB/16.0 MiB (2.0 MiB/s) with 52 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file189.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file189.csv\n",
      "Completed 1.4 MiB/16.0 MiB (2.0 MiB/s) with 51 file(s) remaining\n",
      "Completed 1.5 MiB/16.0 MiB (2.2 MiB/s) with 51 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1764.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1764.csv\n",
      "Completed 1.5 MiB/16.0 MiB (2.2 MiB/s) with 50 file(s) remaining\n",
      "Completed 1.6 MiB/16.0 MiB (2.3 MiB/s) with 50 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file252.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file252.csv\n",
      "Completed 1.6 MiB/16.0 MiB (2.3 MiB/s) with 49 file(s) remaining\n",
      "Completed 1.7 MiB/16.0 MiB (2.5 MiB/s) with 49 file(s) remaining\n",
      "Completed 1.8 MiB/16.0 MiB (2.6 MiB/s) with 49 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file315.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file315.csv\n",
      "Completed 1.8 MiB/16.0 MiB (2.6 MiB/s) with 48 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1827.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1827.csv\n",
      "Completed 1.8 MiB/16.0 MiB (2.6 MiB/s) with 47 file(s) remaining\n",
      "Completed 1.9 MiB/16.0 MiB (2.7 MiB/s) with 47 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1890.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1890.csv\n",
      "Completed 1.9 MiB/16.0 MiB (2.7 MiB/s) with 46 file(s) remaining\n",
      "Completed 2.0 MiB/16.0 MiB (2.7 MiB/s) with 46 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file441.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file441.csv\n",
      "Completed 2.0 MiB/16.0 MiB (2.7 MiB/s) with 45 file(s) remaining\n",
      "Completed 2.1 MiB/16.0 MiB (2.9 MiB/s) with 45 file(s) remaining\n",
      "Completed 2.2 MiB/16.0 MiB (3.0 MiB/s) with 45 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file504.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file504.csv\n",
      "Completed 2.2 MiB/16.0 MiB (3.0 MiB/s) with 44 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file378.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file378.csv\n",
      "Completed 2.2 MiB/16.0 MiB (3.0 MiB/s) with 43 file(s) remaining\n",
      "Completed 2.3 MiB/16.0 MiB (3.0 MiB/s) with 43 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file567.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file567.csv\n",
      "Completed 2.3 MiB/16.0 MiB (3.0 MiB/s) with 42 file(s) remaining\n",
      "Completed 2.4 MiB/16.0 MiB (3.1 MiB/s) with 42 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file693.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file693.csv\n",
      "Completed 2.4 MiB/16.0 MiB (3.1 MiB/s) with 41 file(s) remaining\n",
      "Completed 2.5 MiB/16.0 MiB (3.3 MiB/s) with 41 file(s) remaining\n",
      "Completed 2.6 MiB/16.0 MiB (3.4 MiB/s) with 41 file(s) remaining\n",
      "Completed 2.7 MiB/16.0 MiB (3.5 MiB/s) with 41 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file63.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file63.csv\n",
      "Completed 2.7 MiB/16.0 MiB (3.5 MiB/s) with 40 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file756.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file756.csv\n",
      "Completed 2.7 MiB/16.0 MiB (3.5 MiB/s) with 39 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file630.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file630.csv\n",
      "Completed 2.7 MiB/16.0 MiB (3.5 MiB/s) with 38 file(s) remaining\n",
      "Completed 2.8 MiB/16.0 MiB (3.6 MiB/s) with 38 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file882.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file882.csv\n",
      "Completed 2.8 MiB/16.0 MiB (3.6 MiB/s) with 37 file(s) remaining\n",
      "Completed 2.9 MiB/16.0 MiB (3.7 MiB/s) with 37 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file945.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file945.csv\n",
      "Completed 2.9 MiB/16.0 MiB (3.7 MiB/s) with 36 file(s) remaining\n",
      "Completed 3.2 MiB/16.0 MiB (4.0 MiB/s) with 36 file(s) remaining\n",
      "Completed 3.4 MiB/16.0 MiB (4.3 MiB/s) with 36 file(s) remaining\n",
      "Completed 3.5 MiB/16.0 MiB (4.3 MiB/s) with 36 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file819.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file819.csv\n",
      "Completed 3.5 MiB/16.0 MiB (4.3 MiB/s) with 35 file(s) remaining\n",
      "Completed 3.8 MiB/16.0 MiB (4.5 MiB/s) with 35 file(s) remaining\n",
      "Completed 4.0 MiB/16.0 MiB (4.7 MiB/s) with 35 file(s) remaining\n",
      "Completed 4.3 MiB/16.0 MiB (5.0 MiB/s) with 35 file(s) remaining\n",
      "Completed 4.5 MiB/16.0 MiB (5.2 MiB/s) with 35 file(s) remaining\n",
      "Completed 4.8 MiB/16.0 MiB (5.4 MiB/s) with 35 file(s) remaining\n",
      "Completed 5.0 MiB/16.0 MiB (5.7 MiB/s) with 35 file(s) remaining\n",
      "Completed 5.1 MiB/16.0 MiB (5.7 MiB/s) with 35 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file0.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file0.csv\n",
      "Completed 5.1 MiB/16.0 MiB (5.7 MiB/s) with 34 file(s) remaining\n",
      "Completed 5.4 MiB/16.0 MiB (5.9 MiB/s) with 34 file(s) remaining\n",
      "Completed 5.5 MiB/16.0 MiB (6.0 MiB/s) with 34 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file1020.csv to s3://xy-mp-pipeline/data/covid-csv/training/file1020.csv\n",
      "Completed 5.5 MiB/16.0 MiB (6.0 MiB/s) with 33 file(s) remaining\n",
      "Completed 5.7 MiB/16.0 MiB (6.1 MiB/s) with 33 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file0.csv to s3://xy-mp-pipeline/data/covid-csv/training/file0.csv\n",
      "Completed 5.7 MiB/16.0 MiB (6.1 MiB/s) with 32 file(s) remaining\n",
      "Completed 5.9 MiB/16.0 MiB (6.2 MiB/s) with 32 file(s) remaining\n",
      "Completed 6.1 MiB/16.0 MiB (6.4 MiB/s) with 32 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file1275.csv to s3://xy-mp-pipeline/data/covid-csv/training/file1275.csv\n",
      "Completed 6.1 MiB/16.0 MiB (6.4 MiB/s) with 31 file(s) remaining\n",
      "Completed 6.2 MiB/16.0 MiB (6.5 MiB/s) with 31 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file2805.csv to s3://xy-mp-pipeline/data/covid-csv/training/file2805.csv\n",
      "Completed 6.2 MiB/16.0 MiB (6.5 MiB/s) with 30 file(s) remaining\n",
      "Completed 6.5 MiB/16.0 MiB (6.7 MiB/s) with 30 file(s) remaining\n",
      "Completed 6.6 MiB/16.0 MiB (6.8 MiB/s) with 30 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file1785.csv to s3://xy-mp-pipeline/data/covid-csv/training/file1785.csv\n",
      "Completed 6.6 MiB/16.0 MiB (6.8 MiB/s) with 29 file(s) remaining\n",
      "Completed 6.9 MiB/16.0 MiB (7.0 MiB/s) with 29 file(s) remaining\n",
      "Completed 7.0 MiB/16.0 MiB (7.2 MiB/s) with 29 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file2295.csv to s3://xy-mp-pipeline/data/covid-csv/training/file2295.csv\n",
      "Completed 7.0 MiB/16.0 MiB (7.2 MiB/s) with 28 file(s) remaining\n",
      "Completed 7.2 MiB/16.0 MiB (7.2 MiB/s) with 28 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file1530.csv to s3://xy-mp-pipeline/data/covid-csv/training/file1530.csv\n",
      "Completed 7.2 MiB/16.0 MiB (7.2 MiB/s) with 27 file(s) remaining\n",
      "Completed 7.3 MiB/16.0 MiB (7.3 MiB/s) with 27 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file2040.csv to s3://xy-mp-pipeline/data/covid-csv/training/file2040.csv\n",
      "Completed 7.3 MiB/16.0 MiB (7.3 MiB/s) with 26 file(s) remaining\n",
      "Completed 7.6 MiB/16.0 MiB (7.5 MiB/s) with 26 file(s) remaining\n",
      "Completed 7.8 MiB/16.0 MiB (7.7 MiB/s) with 26 file(s) remaining\n",
      "Completed 8.0 MiB/16.0 MiB (7.8 MiB/s) with 26 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file255.csv to s3://xy-mp-pipeline/data/covid-csv/training/file255.csv\n",
      "Completed 8.0 MiB/16.0 MiB (7.8 MiB/s) with 25 file(s) remaining\n",
      "Completed 8.2 MiB/16.0 MiB (8.0 MiB/s) with 25 file(s) remaining\n",
      "Completed 8.4 MiB/16.0 MiB (8.0 MiB/s) with 25 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file2550.csv to s3://xy-mp-pipeline/data/covid-csv/training/file2550.csv\n",
      "Completed 8.4 MiB/16.0 MiB (8.0 MiB/s) with 24 file(s) remaining\n",
      "Completed 8.6 MiB/16.0 MiB (8.2 MiB/s) with 24 file(s) remaining\n",
      "Completed 8.9 MiB/16.0 MiB (8.3 MiB/s) with 24 file(s) remaining\n",
      "Completed 9.1 MiB/16.0 MiB (8.4 MiB/s) with 24 file(s) remaining\n",
      "Completed 9.3 MiB/16.0 MiB (8.5 MiB/s) with 24 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file3060.csv to s3://xy-mp-pipeline/data/covid-csv/training/file3060.csv\n",
      "Completed 9.3 MiB/16.0 MiB (8.5 MiB/s) with 23 file(s) remaining\n",
      "Completed 9.5 MiB/16.0 MiB (8.7 MiB/s) with 23 file(s) remaining\n",
      "Completed 9.7 MiB/16.0 MiB (8.7 MiB/s) with 23 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file3315.csv to s3://xy-mp-pipeline/data/covid-csv/training/file3315.csv\n",
      "Completed 9.7 MiB/16.0 MiB (8.7 MiB/s) with 22 file(s) remaining\n",
      "Completed 9.9 MiB/16.0 MiB (8.9 MiB/s) with 22 file(s) remaining\n",
      "Completed 10.1 MiB/16.0 MiB (8.9 MiB/s) with 22 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file3570.csv to s3://xy-mp-pipeline/data/covid-csv/training/file3570.csv\n",
      "Completed 10.1 MiB/16.0 MiB (8.9 MiB/s) with 21 file(s) remaining\n",
      "Completed 10.2 MiB/16.0 MiB (9.0 MiB/s) with 21 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file3825.csv to s3://xy-mp-pipeline/data/covid-csv/training/file3825.csv\n",
      "Completed 10.2 MiB/16.0 MiB (9.0 MiB/s) with 20 file(s) remaining\n",
      "Completed 10.4 MiB/16.0 MiB (9.1 MiB/s) with 20 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file4080.csv to s3://xy-mp-pipeline/data/covid-csv/training/file4080.csv\n",
      "Completed 10.4 MiB/16.0 MiB (9.1 MiB/s) with 19 file(s) remaining\n",
      "Completed 10.7 MiB/16.0 MiB (9.3 MiB/s) with 19 file(s) remaining\n",
      "Completed 10.8 MiB/16.0 MiB (9.3 MiB/s) with 19 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file4845.csv to s3://xy-mp-pipeline/data/covid-csv/training/file4845.csv\n",
      "Completed 10.8 MiB/16.0 MiB (9.3 MiB/s) with 18 file(s) remaining\n",
      "Completed 11.1 MiB/16.0 MiB (9.5 MiB/s) with 18 file(s) remaining\n",
      "Completed 11.2 MiB/16.0 MiB (9.5 MiB/s) with 18 file(s) remaining\n",
      "Completed 11.4 MiB/16.0 MiB (9.6 MiB/s) with 18 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file4590.csv to s3://xy-mp-pipeline/data/covid-csv/training/file4590.csv\n",
      "Completed 11.4 MiB/16.0 MiB (9.6 MiB/s) with 17 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file4335.csv to s3://xy-mp-pipeline/data/covid-csv/training/file4335.csv\n",
      "Completed 11.4 MiB/16.0 MiB (9.6 MiB/s) with 16 file(s) remaining\n",
      "Completed 11.6 MiB/16.0 MiB (9.8 MiB/s) with 16 file(s) remaining\n",
      "Completed 11.8 MiB/16.0 MiB (9.8 MiB/s) with 16 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file5100.csv to s3://xy-mp-pipeline/data/covid-csv/training/file5100.csv\n",
      "Completed 11.8 MiB/16.0 MiB (9.8 MiB/s) with 15 file(s) remaining\n",
      "Completed 11.9 MiB/16.0 MiB (9.9 MiB/s) with 15 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file510.csv to s3://xy-mp-pipeline/data/covid-csv/training/file510.csv\n",
      "Completed 11.9 MiB/16.0 MiB (9.9 MiB/s) with 14 file(s) remaining\n",
      "Completed 12.2 MiB/16.0 MiB (9.9 MiB/s) with 14 file(s) remaining\n",
      "Completed 12.4 MiB/16.0 MiB (10.0 MiB/s) with 14 file(s) remaining\n",
      "Completed 12.7 MiB/16.0 MiB (10.2 MiB/s) with 14 file(s) remaining\n",
      "Completed 12.9 MiB/16.0 MiB (10.1 MiB/s) with 14 file(s) remaining\n",
      "Completed 13.1 MiB/16.0 MiB (10.3 MiB/s) with 14 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file5355.csv to s3://xy-mp-pipeline/data/covid-csv/training/file5355.csv\n",
      "Completed 13.1 MiB/16.0 MiB (10.3 MiB/s) with 13 file(s) remaining\n",
      "Completed 13.3 MiB/16.0 MiB (10.4 MiB/s) with 13 file(s) remaining\n",
      "Completed 13.5 MiB/16.0 MiB (10.4 MiB/s) with 13 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file5610.csv to s3://xy-mp-pipeline/data/covid-csv/training/file5610.csv\n",
      "Completed 13.5 MiB/16.0 MiB (10.4 MiB/s) with 12 file(s) remaining\n",
      "Completed 13.7 MiB/16.0 MiB (10.5 MiB/s) with 12 file(s) remaining\n",
      "Completed 14.0 MiB/16.0 MiB (10.7 MiB/s) with 12 file(s) remaining\n",
      "Completed 14.1 MiB/16.0 MiB (10.5 MiB/s) with 12 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file6375.csv to s3://xy-mp-pipeline/data/covid-csv/training/file6375.csv\n",
      "Completed 14.1 MiB/16.0 MiB (10.5 MiB/s) with 11 file(s) remaining\n",
      "Completed 14.4 MiB/16.0 MiB (10.7 MiB/s) with 11 file(s) remaining\n",
      "Completed 14.6 MiB/16.0 MiB (10.8 MiB/s) with 11 file(s) remaining\n",
      "Completed 14.7 MiB/16.0 MiB (10.7 MiB/s) with 11 file(s) remaining\n",
      "Completed 14.9 MiB/16.0 MiB (10.8 MiB/s) with 11 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file6120.csv to s3://xy-mp-pipeline/data/covid-csv/training/file6120.csv\n",
      "Completed 14.9 MiB/16.0 MiB (10.8 MiB/s) with 10 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file5865.csv to s3://xy-mp-pipeline/data/covid-csv/training/file5865.csv\n",
      "Completed 14.9 MiB/16.0 MiB (10.8 MiB/s) with 9 file(s) remaining\n",
      "Completed 14.9 MiB/16.0 MiB (10.8 MiB/s) with 9 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file8160.csv to s3://xy-mp-pipeline/data/covid-csv/training/file8160.csv\n",
      "Completed 14.9 MiB/16.0 MiB (10.8 MiB/s) with 8 file(s) remaining\n",
      "Completed 15.0 MiB/16.0 MiB (10.8 MiB/s) with 8 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file7905.csv to s3://xy-mp-pipeline/data/covid-csv/training/file7905.csv\n",
      "Completed 15.0 MiB/16.0 MiB (10.8 MiB/s) with 7 file(s) remaining\n",
      "Completed 15.2 MiB/16.0 MiB (10.8 MiB/s) with 7 file(s) remaining\n",
      "Completed 15.3 MiB/16.0 MiB (10.9 MiB/s) with 7 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file7140.csv to s3://xy-mp-pipeline/data/covid-csv/training/file7140.csv\n",
      "Completed 15.3 MiB/16.0 MiB (10.9 MiB/s) with 6 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file6630.csv to s3://xy-mp-pipeline/data/covid-csv/training/file6630.csv\n",
      "Completed 15.3 MiB/16.0 MiB (10.9 MiB/s) with 5 file(s) remaining\n",
      "Completed 15.5 MiB/16.0 MiB (11.0 MiB/s) with 5 file(s) remaining\n",
      "Completed 15.6 MiB/16.0 MiB (11.1 MiB/s) with 5 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file6885.csv to s3://xy-mp-pipeline/data/covid-csv/training/file6885.csv\n",
      "Completed 15.6 MiB/16.0 MiB (11.1 MiB/s) with 4 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file765.csv to s3://xy-mp-pipeline/data/covid-csv/training/file765.csv\n",
      "Completed 15.6 MiB/16.0 MiB (11.1 MiB/s) with 3 file(s) remaining\n",
      "Completed 15.7 MiB/16.0 MiB (10.9 MiB/s) with 3 file(s) remaining\n",
      "upload: data\\covid-csv\\testing\\file1008.csv to s3://xy-mp-pipeline/data/covid-csv/testing/file1008.csv\n",
      "Completed 15.7 MiB/16.0 MiB (10.9 MiB/s) with 2 file(s) remaining\n",
      "Completed 15.9 MiB/16.0 MiB (11.0 MiB/s) with 2 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file7395.csv to s3://xy-mp-pipeline/data/covid-csv/training/file7395.csv\n",
      "Completed 15.9 MiB/16.0 MiB (11.0 MiB/s) with 1 file(s) remaining\n",
      "Completed 16.0 MiB/16.0 MiB (11.1 MiB/s) with 1 file(s) remaining\n",
      "upload: data\\covid-csv\\training\\file7650.csv to s3://xy-mp-pipeline/data/covid-csv/training/file7650.csv\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df)\n",
    "train_len, test_len = len(train_df), len(test_df)\n",
    "write_csvs(train_df, 'training', OUTPUT_PATH, BATCHES)\n",
    "write_csvs(test_df, 'testing', OUTPUT_PATH, BATCHES)\n",
    "\n",
    "write_files_to_s3(OUTPUT_PATH, BUCKET_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create benchmarking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.iter import IterableWrapper, IterDataPipe\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TextDataset(IterDataPipe):\n",
    "    def __init__(self, s3_urls, tokenizer, length):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.url_wrapper = s3_urls\n",
    "        self.len = length\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _, file in self.url_wrapper.load_files_by_s3():\n",
    "            temp = pd.read_csv(file)\n",
    "            label = torch.from_numpy(temp['outcome'].values)\n",
    "            embedded = [self.tokenizer(t, padding='max_length', max_length=100, truncation=True, return_tensors='pt') for t in temp['headlines']]\n",
    "\n",
    "            input_ids = torch.cat([e['input_ids'] for e in embedded], dim=0)\n",
    "            attention_mask = torch.cat([e['attention_mask'] for e in embedded], dim=0)\n",
    "            yield input_ids, attention_mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class FakeNewsClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = torch.nn.Dropout(0.25)\n",
    "        self.linear = torch.nn.Linear(768, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        return self.sigmoid(linear_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: torch.nn.Module, train_data_url: str, test_data_url: str, train_len: int, test_len:int, epochs: int, lr: float):\n",
    "    # Prepare dataloaders\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    train_s3_url = IterableWrapper([train_data_url]).list_files_by_s3().shuffle().sharding_filter()\n",
    "    test_s3_url = IterableWrapper([test_data_url]).list_files_by_s3().shuffle().sharding_filter()\n",
    "\n",
    "    train_df = TextDataset(train_s3_url, tokenizer, train_len)\n",
    "    test_df = TextDataset(test_s3_url, tokenizer, test_len)\n",
    "\n",
    "    train_loader = DataLoader(train_df, batch_size=1, shuffle=True, collate_fn=lambda x: x)\n",
    "    test_loader = DataLoader(test_df, batch_size=1, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "    # Config device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Config optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = torch.nn.BCELoss()\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        training_acc = 0.0\n",
    "\n",
    "        for data in train_loader:\n",
    "            input_ids, mask, label = data[0]\n",
    "            label = label.unsqueeze(1).to(device)\n",
    "            mask = mask.to(device)\n",
    "            input_ids = input_ids.squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_ids, mask)\n",
    "\n",
    "            loss = loss_function(output, label.float())\n",
    "            training_loss += loss.item()\n",
    "\n",
    "            acc = (output.argmax(1) == label).sum().item()\n",
    "            training_acc += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        validation_loss = 0.0\n",
    "        validation_acc = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                input_ids, mask, label = data[0]\n",
    "                label = label.unsqueeze(1).to(device)\n",
    "                mask = mask.to(device)\n",
    "                input_ids = input_ids.squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_ids, mask)\n",
    "\n",
    "                loss = loss_function(output, label.float())\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                acc = (output.argmax(1) == label).sum().item()\n",
    "                validation_acc += acc\n",
    "        print(f'Epoch: {epoch+1}/{epochs} | Training loss: {training_loss/len(train_loader):.3f} | Training acc: {training_acc/len(train_loader):.3f} | Validation loss: {validation_loss/len(test_loader):.3f} | Validation acc: {validation_acc/len(test_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_IterDataPipeSerializationWrapper instance doesn't have valid length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Xin Yu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\datapipe.py:351\u001b[0m, in \u001b[0;36m_DataPipeSerializationWrapper.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 351\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_datapipe)\n\u001b[0;32m    352\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'TextDataset' has no len()",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Xin Yu\\Desktop\\data_science\\nlp-pipeline\\test.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m TRAIN_S3_URL \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms3://\u001b[39m\u001b[39m{\u001b[39;00mBUCKET_NAME\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mOUTPUT_PATH\u001b[39m}\u001b[39;00m\u001b[39m/training/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m TEST_S3_URL \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms3://\u001b[39m\u001b[39m{\u001b[39;00mBUCKET_NAME\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mOUTPUT_PATH\u001b[39m}\u001b[39;00m\u001b[39m/testing/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_model(model, TRAIN_S3_URL, TEST_S3_URL, EPOCHS, LR)\n",
      "\u001b[1;32mc:\\Users\\Xin Yu\\Desktop\\data_science\\nlp-pipeline\\test.ipynb Cell 13\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_data_url, test_data_url, epochs, lr)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X23sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         acc \u001b[39m=\u001b[39m (output\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m label)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X23sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         validation_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m acc\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X23sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m | Training loss: \u001b[39m\u001b[39m{\u001b[39;00mtraining_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_loader)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Training acc: \u001b[39m\u001b[39m{\u001b[39;00mtraining_acc\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_loader)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Validation loss: \u001b[39m\u001b[39m{\u001b[39;00mvalidation_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(test_loader)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Validation acc: \u001b[39m\u001b[39m{\u001b[39;00mvalidation_acc\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(test_loader)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Xin Yu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:477\u001b[0m, in \u001b[0;36mDataLoader.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m    460\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m    461\u001b[0m         \u001b[39m# NOTE [ IterableDataset and __len__ ]\u001b[39;00m\n\u001b[0;32m    462\u001b[0m         \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    475\u001b[0m \n\u001b[0;32m    476\u001b[0m         \u001b[39m# Cannot statically verify that dataset is Sized\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m         length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset)  \u001b[39m# type: ignore[assignment, arg-type]\u001b[39;00m\n\u001b[0;32m    478\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# IterableDataset doesn't allow custom sampler or batch_sampler\u001b[39;00m\n\u001b[0;32m    479\u001b[0m             \u001b[39mfrom\u001b[39;00m \u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m ceil\n",
      "File \u001b[1;32mc:\\Users\\Xin Yu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\datapipe.py:353\u001b[0m, in \u001b[0;36m_DataPipeSerializationWrapper.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_datapipe)\n\u001b[0;32m    352\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    354\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m instance doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have valid length\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    355\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: _IterDataPipeSerializationWrapper instance doesn't have valid length"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "model = FakeNewsClassifier()\n",
    "LR = 5e-6\n",
    "TRAIN_S3_URL = f's3://{BUCKET_NAME}/{OUTPUT_PATH}/training/'\n",
    "TEST_S3_URL = f's3://{BUCKET_NAME}/{OUTPUT_PATH}/testing/'\n",
    "\n",
    "train_model(model, TRAIN_S3_URL, TEST_S3_URL, train_len, test_len, EPOCHS, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Xin Yu\\Desktop\\data_science\\nlp-pipeline\\test.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_df, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# z, y, x  = data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(data[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_s3_url = IterableWrapper([TRAIN_S3_URL]).list_files_by_s3().shuffle().sharding_filter()\n",
    "train_df = TextDataset(train_s3_url, tokenizer)\n",
    "train_loader = DataLoader(train_df, batch_size=1, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "for data in train_loader:\n",
    "    # z, y, x  = data\n",
    "    print(data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([255, 100]), torch.Size([255, 100]), torch.Size([255]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z = data[0]\n",
    "x.shape, y.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([255, 1])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Xin Yu\\Desktop\\data_science\\nlp-pipeline\\test.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/test.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mstack(df\u001b[39m.\u001b[39;49membeddings)\n",
      "\u001b[1;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Series"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
