{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Modeling (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdata.datapipes.iter import IterableWrapper, IterDataPipe\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'xy-mp-pipeline'\n",
    "OUTPUT_PATH = 'data/covid-csv'\n",
    "N_SAMPLES = 19454\n",
    "TRAIN_FILES = N_SAMPLES * 4 // 5 // 16 + 1\n",
    "TEST_FILES = N_SAMPLES // 5 // 16 + 1\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_S3_URL = f's3://{BUCKET_NAME}/{OUTPUT_PATH}/training/'\n",
    "TEST_S3_URL = f's3://{BUCKET_NAME}/{OUTPUT_PATH}/testing/'\n",
    "TEST_DATASET_SIZE = N_SAMPLES // 5\n",
    "TRAIN_DATASET_SIZE = N_SAMPLES - TEST_DATASET_SIZE\n",
    "MODEL_OUTPUT_PATH = 'assets/model'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data pipe\n",
    "I'm sure most of PyTorch users are already familiar with Datasets, it is a convenient way to load data into memory. This time, we will use a different approach to load data into memory, which is called data pipe. Data pipe is a new feature introduced in PyTorch 1.8.0, it is a new way to load data into memory, and it is more flexible than Datasets. \n",
    "\n",
    "The main benefit for this particular project is that we can load data from a cloud bucket, one batch at a time, and we can also do some preprocessing on the fly. This is very useful when we have a large dataset and we don't want to load all the data into memory at once.\n",
    "\n",
    "With this, we can also easily perform data parallel training, which is a very useful technique when we have a large dataset and we want to train our model faster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall the dataframe**\n",
    ">\n",
    "    RangeIndex: 19454 entries, 0 to 19453\n",
    "    Data columns (total 22 columns):\n",
    "    #   Column           Non-Null Count  Dtype \n",
    "    ---  ------           --------------  ----- \n",
    "    0   headlines        19454 non-null  object\n",
    "    1   length           19454 non-null  int64 \n",
    "    2   has_num          19454 non-null  bool  \n",
    "    3   ner_percent      19454 non-null  int64 \n",
    "    4   ner_quantity     19454 non-null  int64 \n",
    "    5   ner_law          19454 non-null  int64 \n",
    "    6   ner_person       19454 non-null  int64 \n",
    "    7   ner_product      19454 non-null  int64 \n",
    "    8   ner_gpe          19454 non-null  int64 \n",
    "    9   ner_work_of_art  19454 non-null  int64 \n",
    "    10  ner_date         19454 non-null  int64 \n",
    "    11  ner_time         19454 non-null  int64 \n",
    "    12  ner_cardinal     19454 non-null  int64 \n",
    "    13  ner_org          19454 non-null  int64 \n",
    "    14  ner_money        19454 non-null  int64 \n",
    "    15  ner_language     19454 non-null  int64 \n",
    "    16  ner_ordinal      19454 non-null  int64 \n",
    "    17  ner_event        19454 non-null  int64 \n",
    "    18  ner_loc          19454 non-null  int64 \n",
    "    19  ner_fac          19454 non-null  int64 \n",
    "    20  ner_norp         19454 non-null  int64 \n",
    "    21  outcome          19454 non-null  int64 \n",
    "    dtypes: bool(1), int64(20), object(1)\n",
    "    memory usage: 3.1+ MB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we want to pass the text into a pre-trained BERT model, at the same time, we also want to pass the other features into a fully connected layer. To do this, we need to create a custom data pipe that gives the input to BERT, tabular features, and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(IterDataPipe):\n",
    "    def __init__(self, s3_urls, tokenizer, num_files):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.url_wrapper = s3_urls\n",
    "        self.num_files = num_files\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _, file in self.url_wrapper.load_files_by_s3():\n",
    "            temp = pd.read_csv(file)\n",
    "            label = torch.from_numpy(temp['outcome'].values)\n",
    "\n",
    "            headlines = temp.headlines.values\n",
    "\n",
    "            # Tabular features\n",
    "            tabular_input = [torch.from_numpy(temp[col].values).to(torch.float32).squeeze() for col in temp.columns if col not in ['outcome', 'headlines']]\n",
    "            yield headlines, tabular_input, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "\n",
    "\n",
    "            # # For BERT model\n",
    "            # bert_input = []\n",
    "            # embedded = [self.tokenizer(t, padding='max_length', max_length=100, truncation=True, return_tensors='pt') for t in temp['headlines']]\n",
    "            # bert_input.append(torch.cat([e['input_ids'] for e in embedded], dim=0))\n",
    "            # bert_input.append(torch.cat([e['attention_mask'] for e in embedded], dim=0))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class FakeNewsClassifier(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout_1 = torch.nn.Dropout(0.25)\n",
    "        self.linear = torch.nn.Linear(768, 12)\n",
    "        self.dropout_2 = torch.nn.Dropout(0.25)\n",
    "        self.final_linear = torch.nn.Linear(32, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.normalize = torch.nn.functional.normalize\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, bert_input: dict, tabular_input: list):\n",
    "        _, pooled_output = self.bert(**bert_input)\n",
    "        dropout_1_output = self.dropout_1(pooled_output)\n",
    "        linear_output = self.linear(dropout_1_output)\n",
    "        relu_output = self.relu(linear_output)\n",
    "        norm1 = self.normalize(relu_output, p=2, dim=1)\n",
    "        norm2 = self.normalize(tabular_input, p=2, dim=1)\n",
    "        combined_output = torch.cat([norm1, norm2], dim=1)\n",
    "        dropout_2_output = self.dropout_2(combined_output)\n",
    "        final_output = self.final_linear(dropout_2_output)\n",
    "        return self.sigmoid(final_output)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the model looks like"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/model_architecture.png \"Model Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: torch.nn.Module, train_data_url: str, test_data_url: str, train_len: int, test_len:int, train_file_len: int, test_file_len: int, epochs: int, lr: float):\n",
    "    # Prepare dataloaders\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    train_s3_url = IterableWrapper([train_data_url]).list_files_by_s3().shuffle().sharding_filter()\n",
    "    test_s3_url = IterableWrapper([test_data_url]).list_files_by_s3().shuffle().sharding_filter()\n",
    "\n",
    "    train_df = TextDataset(train_s3_url, tokenizer, train_file_len)\n",
    "    test_df = TextDataset(test_s3_url, tokenizer, test_file_len)\n",
    "\n",
    "    train_loader = DataLoader(train_df, batch_size=1, shuffle=True)\n",
    "    test_loader = DataLoader(test_df, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Config device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Config optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = torch.nn.BCELoss()\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        training_acc = 0.0\n",
    "\n",
    "        for bert_input, tabular_input, label in tqdm(train_loader):\n",
    "            bert_input = {\n",
    "                'input_ids': bert_input[0].squeeze().to(device),\n",
    "                'attention_mask': bert_input[1].squeeze().to(device),\n",
    "                'return_dict': False\n",
    "            }\n",
    "            tabular_input = torch.cat(tabular_input).T.to(device)\n",
    "            label = label.T.to(device)\n",
    "\n",
    "            output = model(bert_input, tabular_input)\n",
    "\n",
    "            loss = loss_function(output, label.float())\n",
    "            training_loss += loss.item()\n",
    "\n",
    "            # get acc of signmoid output\n",
    "            acc = (output[0].round() == label).sum().item()\n",
    "            training_acc += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        validation_loss = 0.0\n",
    "        validation_acc = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for bert_input, tabular_input, label in test_loader:\n",
    "                bert_input = {\n",
    "                    'input_ids': bert_input[0].squeeze().to(device),\n",
    "                    'attention_mask': bert_input[1].squeeze().to(device),\n",
    "                    'return_dict': False\n",
    "                }\n",
    "                tabular_input = torch.cat(tabular_input).T.to(device)\n",
    "                label = label.T.to(device)\n",
    "\n",
    "                output = model(bert_input, tabular_input)\n",
    "\n",
    "                loss = loss_function(output, label.float())\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # get acc of signmoid output\n",
    "                acc = (output[0].round() == label).sum().item()\n",
    "                validation_acc += acc\n",
    "        print(f'Epoch: {epoch+1}/{epochs} | Training loss: {training_loss/train_len:.3f} | Training acc: {training_acc/train_len:.3f} | Validation loss: {validation_loss/test_len:.3f} | Validation acc: {validation_acc/test_len:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 973/973 [40:09<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 | Training loss: 0.035 | Training acc: 0.849 | Validation loss: 0.036 | Validation acc: 0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 973/973 [40:34<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5 | Training loss: 0.033 | Training acc: 0.916 | Validation loss: 0.035 | Validation acc: 0.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 973/973 [40:58<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5 | Training loss: 0.033 | Training acc: 0.936 | Validation loss: 0.035 | Validation acc: 0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 973/973 [40:32<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5 | Training loss: 0.032 | Training acc: 0.951 | Validation loss: 0.035 | Validation acc: 0.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 973/973 [40:08<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5 | Training loss: 0.032 | Training acc: 0.954 | Validation loss: 0.033 | Validation acc: 0.941\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory assets/model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Xin Yu\\Desktop\\data_science\\nlp-pipeline\\model.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(MODEL_OUTPUT_PATH):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(MODEL_OUTPUT_PATH)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), MODEL_OUTPUT_PATH \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/baseline.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Xin%20Yu/Desktop/data_science/nlp-pipeline/model.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mModel saved to:  \u001b[39m\u001b[39m'\u001b[39m, MODEL_OUTPUT_PATH)\n",
      "File \u001b[1;32mc:\\Users\\Xin Yu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Xin Yu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[1;32mc:\\Users\\Xin Yu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory assets/model does not exist."
     ]
    }
   ],
   "source": [
    "model = FakeNewsClassifier()\n",
    "EPOCHS = 5\n",
    "LR = 5e-6\n",
    "\n",
    "\n",
    "train_model(\n",
    "    model, \n",
    "    TRAIN_S3_URL, \n",
    "    TEST_S3_URL, \n",
    "    TRAIN_DATASET_SIZE, \n",
    "    TEST_DATASET_SIZE, \n",
    "    TRAIN_FILES, \n",
    "    TEST_FILES,\n",
    "    EPOCHS, \n",
    "    LR\n",
    ")\n",
    "\n",
    "import os\n",
    "if not os.path.exists(MODEL_OUTPUT_PATH):\n",
    "    os.makedirs(MODEL_OUTPUT_PATH)\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_OUTPUT_PATH + '/baseline.pth')\n",
    "print('Model saved to:  ', MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(MODEL_OUTPUT_PATH):\n",
    "    os.makedirs(MODEL_OUTPUT_PATH)\n",
    "torch.save(model.state_dict(),MODEL_OUTPUT_PATH + '/baseline.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assets/model/baseline.pth'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_OUTPUT_PATH + '/baseline.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
